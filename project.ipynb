{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aPsNrATuN58S",
    "ExecuteTime": {
     "end_time": "2024-05-17T14:25:45.441585Z",
     "start_time": "2024-05-17T14:25:43.720615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# This cell makes sure modules are auto-loaded when you change external python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfLW9R8gN58V",
    "outputId": "8f6c6558-4ff4-4b2a-9b2e-ec4e56d13393",
    "ExecuteTime": {
     "end_time": "2024-05-17T12:53:22.485575Z",
     "start_time": "2024-05-17T12:53:20.362907Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# If working in Colab, mount your drive\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m drive\n\u001B[0;32m      3\u001B[0m drive\u001B[38;5;241m.\u001B[39mmount(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/content/drive\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Direct to your project folder.\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# If working in Colab, mount your drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Direct to your project folder.\n",
    "%cd /content/drive/Othercomputers/Computer1/dl-robocop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "knGZpRGNN58Y",
    "outputId": "33b03c9d-496d-4be0-c120-f0936e72cfcb",
    "ExecuteTime": {
     "end_time": "2024-05-17T12:53:36.104973Z",
     "start_time": "2024-05-17T12:53:24.174523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.25.2 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from -r requirements.txt (line 1)) (1.25.2)\n",
      "Requirement already satisfied: tqdm==4.66.2 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from -r requirements.txt (line 2)) (4.66.2)\n",
      "Requirement already satisfied: torch==2.1.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from -r requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: datasets==2.17.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from -r requirements.txt (line 4)) (2.17.1)\n",
      "Requirement already satisfied: transformers==4.37.2 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from -r requirements.txt (line 5)) (4.37.2)\n",
      "Requirement already satisfied: evaluate==0.4.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: matplotlib==3.7.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from -r requirements.txt (line 7)) (3.7.1)\n",
      "Requirement already satisfied: tensorboard==2.15.2 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from -r requirements.txt (line 8)) (2.15.2)\n",
      "Requirement already satisfied: bert-score in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from -r requirements.txt (line 9)) (0.3.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from tqdm==4.66.2->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from torch==2.1.0->-r requirements.txt (line 3)) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from torch==2.1.0->-r requirements.txt (line 3)) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from torch==2.1.0->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from torch==2.1.0->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from torch==2.1.0->-r requirements.txt (line 3)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from torch==2.1.0->-r requirements.txt (line 3)) (2023.10.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from datasets==2.17.1->-r requirements.txt (line 4)) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from datasets==2.17.1->-r requirements.txt (line 4)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from datasets==2.17.1->-r requirements.txt (line 4)) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from datasets==2.17.1->-r requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from datasets==2.17.1->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from datasets==2.17.1->-r requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from datasets==2.17.1->-r requirements.txt (line 4)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from datasets==2.17.1->-r requirements.txt (line 4)) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from datasets==2.17.1->-r requirements.txt (line 4)) (0.22.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from datasets==2.17.1->-r requirements.txt (line 4)) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from datasets==2.17.1->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from transformers==4.37.2->-r requirements.txt (line 5)) (2024.4.16)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from transformers==4.37.2->-r requirements.txt (line 5)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from transformers==4.37.2->-r requirements.txt (line 5)) (0.4.3)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from evaluate==0.4.1->-r requirements.txt (line 6)) (0.18.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 7)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 7)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 7)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 7)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 7)) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 7)) (6.4.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from tensorboard==2.15.2->-r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from tensorboard==2.15.2->-r requirements.txt (line 8)) (1.62.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from tensorboard==2.15.2->-r requirements.txt (line 8)) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from tensorboard==2.15.2->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from tensorboard==2.15.2->-r requirements.txt (line 8)) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from tensorboard==2.15.2->-r requirements.txt (line 8)) (5.26.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from tensorboard==2.15.2->-r requirements.txt (line 8)) (68.2.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from tensorboard==2.15.2->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from tensorboard==2.15.2->-r requirements.txt (line 8)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from tensorboard==2.15.2->-r requirements.txt (line 8)) (3.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from aiohttp->datasets==2.17.1->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from aiohttp->datasets==2.17.1->-r requirements.txt (line 4)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from aiohttp->datasets==2.17.1->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from aiohttp->datasets==2.17.1->-r requirements.txt (line 4)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from aiohttp->datasets==2.17.1->-r requirements.txt (line 4)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from aiohttp->datasets==2.17.1->-r requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 8)) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 8)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard==2.15.2->-r requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib==3.7.1->-r requirements.txt (line 7)) (3.18.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from markdown>=2.6.8->tensorboard==2.15.2->-r requirements.txt (line 8)) (7.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from pandas->datasets==2.17.1->-r requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from pandas->datasets==2.17.1->-r requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.17.1->-r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.17.1->-r requirements.txt (line 4)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.17.1->-r requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.17.1->-r requirements.txt (line 4)) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard==2.15.2->-r requirements.txt (line 8)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from sympy->torch==2.1.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 8)) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\41786\\onedrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard==2.15.2->-r requirements.txt (line 8)) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installing requirements\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ezu72df7N58Z",
    "ExecuteTime": {
     "end_time": "2024-05-17T14:25:50.515990Z",
     "start_time": "2024-05-17T14:25:49.875926Z"
    }
   },
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "#import evaluate\n",
    "#import gensim\n",
    "import transformers\n",
    "import nltk\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data.dataset import random_split\n",
    "#import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6KO_AXxQpHJ",
    "outputId": "61bb5f43-8dbd-44dc-cd73-85f28beb562b",
    "ExecuteTime": {
     "end_time": "2024-05-17T14:25:58.786809Z",
     "start_time": "2024-05-17T14:25:52.089211Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# BART MODEL (STUDENT)\n",
    "model_name = \"facebook/bart-large\"\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "student_tokenizer.pad_token_id = student_tokenizer.eos_token_id\n",
    "student_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "#model.load_state_dict(torch.load('bart_test.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TR2ykMFCxYC1"
   },
   "outputs": [],
   "source": [
    "# PARADETOX MODEL (TEACHER)\n",
    "#from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "#base_model_name = 'facebook/bart-base'\n",
    "#model_name = 'SkolkovoInstitute/bart-base-detox'\n",
    "#teacher_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "#teacher_model = BartForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0X9j6cGcNKk",
    "outputId": "cc6c96ae-008c-492b-858a-f1f02ea6e519",
    "ExecuteTime": {
     "end_time": "2024-05-17T14:29:03.793028Z",
     "start_time": "2024-05-17T14:25:58.791554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "328d7ff47f334d6aa02093c1921c86cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\41786\\OneDrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\41786\\.cache\\huggingface\\hub\\models--SkolkovoInstitute--roberta_toxicity_classifier. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fef11edc03bb43dc8e21873dcd1f9087"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ef370ba57cd45f9beee09ac5193d20c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84fbc0103e0d40c3af65ec7eaf4867b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad33e1d78e514ec5b212d11161f1d899"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68e98568eb4143c095fd27bd43392a1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# POLITENESS JUDGE\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# load tokenizer and model weights\n",
    "classifier_tokenizer = RobertaTokenizer.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')\n",
    "toxicity_clasifier_model = RobertaForSequenceClassification.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IxELjZ39SQOH",
    "ExecuteTime": {
     "end_time": "2024-05-17T14:29:35.050717Z",
     "start_time": "2024-05-17T14:29:34.112392Z"
    }
   },
   "outputs": [],
   "source": [
    "from helpers import format_data\n",
    "\n",
    "data_path = 'data/'\n",
    "dataset = pd.read_csv(data_path + 'paradetox.tsv', sep='\\t')\n",
    "\n",
    "# Define the maximum length and other parameters\n",
    "max_length = 38\n",
    "tensor_dataset = format_data(max_length, student_tokenizer, dataset)\n",
    "\n",
    "# Define the size of the training set\n",
    "train_size = int(0.8 * len(tensor_dataset))\n",
    "test_size = len(tensor_dataset) - train_size\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_dataset, test_dataset = random_split(tensor_dataset, [train_size, test_size])\n",
    "\n",
    "# Defining parameters for training\n",
    "batch_size = 16\n",
    "lr = 3e-5\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 16, shuffle = False)\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=lr)\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "6Ll4J_e0nfFK",
    "outputId": "e75c61ab-0e0d-445e-a7c4-96113c9f1bb2",
    "ExecuteTime": {
     "end_time": "2024-05-17T15:22:13.631710Z",
     "start_time": "2024-05-17T15:21:32.917635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average train Loss: 5.9861\n",
      "Evaluating on test set...\n",
      "Test Average Loss: 3.5201\n"
     ]
    }
   ],
   "source": [
    "from helpers import evaluate, calculate_toxicity_score, policy_sampling, train_on_paradetox\n",
    "train_on_paradetox(student_model = student_model,\n",
    "                   judge_model = toxicity_clasifier_model,\n",
    "                   tokenizer = student_tokenizer,\n",
    "                   judge_tokenizer = classifier_tokenizer,\n",
    "                   optimizer =optimizer,\n",
    "                   dataloader = train_dataloader,\n",
    "                   test_dataloader = test_dataloader,\n",
    "                   num_epochs = num_epochs,\n",
    "                   alpha = 0.5,\n",
    "                   device = device,\n",
    "                   lambda_tox = 1,\n",
    "                   lambda_bert = 1)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic sentence:   he  had  steel  balls  too !                               \n",
      "Detoxified sentence:                                       \n",
      "tensor([[[-11.9701,  -3.1708,  27.6744,  ...,   1.2927,  -1.6379,  18.4402],\n",
      "         [-17.5852,  -3.7942,  28.5393,  ...,   0.8406,  -2.1964,  13.7490],\n",
      "         [-13.8262,  -3.6494,  31.1646,  ...,   0.6308,  -1.5011,  16.7933],\n",
      "         ...,\n",
      "         [ -4.3621,  -3.5353,  30.5061,  ...,  -1.1026,  -3.2969,  18.2109],\n",
      "         [-17.6482,  -3.9246,  31.9844,  ...,  -2.0402,  -3.3423,  13.9271],\n",
      "         [ -4.1738,  -3.5651,  30.5883,  ...,  -1.0210,  -3.3321,  17.9093]]])\n"
     ]
    }
   ],
   "source": [
    "# TODO: delete this cell (only for testing)\n",
    "# Load .pt file of the trained model\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "trained_model.load_state_dict(torch.load('weights/epoch0.pt'))\n",
    "\n",
    "for batch in test_dataloader:\n",
    "  input_ids, detoxified_ids, input_mask = batch\n",
    "\n",
    "  # Move tensors to the device\n",
    "  input_ids = input_ids.to(device)\n",
    "  detoxified_ids = detoxified_ids.to(device)\n",
    "  input_mask = input_mask.to(device)\n",
    "\n",
    "  # Disable gradient computation\n",
    "  with torch.no_grad():\n",
    "      # Forward pass\n",
    "      model_outputs = trained_model(input_ids=input_ids, attention_mask=input_mask, labels=detoxified_ids)\n",
    "      # Print the text input\n",
    "      print('Toxic sentence: ', ' '.join([student_tokenizer.decode(i, skip_special_tokens = True) for i in input_ids[0]]))\n",
    "      # Print the detoxified output\n",
    "      print('Detoxified sentence: ', ' '.join([student_tokenizer.decode(i, skip_special_tokens = True) for i in model_outputs.logits.argmax(dim=-1)[0]]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T15:31:12.492926Z",
     "start_time": "2024-05-17T15:31:11.302990Z"
    }
   },
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[42], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrained_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mToxic\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:2245\u001B[0m, in \u001B[0;36mBartForCausalLM.forward\u001B[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   2242\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m   2244\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[1;32m-> 2245\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2246\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2247\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2248\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2249\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2250\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2251\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2253\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2254\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2255\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2256\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2258\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2260\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(outputs[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m   2262\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - epfl.ch\\master\\ma2\\dl-robocop\\venv\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1364\u001B[0m, in \u001B[0;36mBartDecoder.forward\u001B[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1362\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m input_ids\n\u001B[0;32m   1363\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m-> 1364\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[43minput_shape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[0;32m   1365\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1366\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m inputs_embeds\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZ2bgTNsxfGe"
   },
   "outputs": [],
   "source": [
    "prompt = \"Rewrite the following toxic input into non-toxic version. Let's break the input down step by step to rewrite the non-toxic version. You should first think about the expanation of why the input text is toxic. Then generate the detoxic output. You must preserve the original meaning as much as possible.\\nInput: \"\n",
    "\n",
    "for (idx,batch) in enumerate(test_dataloader):\n",
    "  #input = prompt+input+\"\\n\"\n",
    "  model.eval()\n",
    "  input_ids, detoxified_ids, mask = batch\n",
    "\n",
    "  input_ids = input_ids.to(device)\n",
    "  detoxified_ids = detoxified_ids.to(device)\n",
    "  mask = mask.to(device)\n",
    "\n",
    "  outputs = model.generate(input_ids = input_ids, attention_mask = mask, max_new_tokens = 50, do_sample=False)\n",
    "  print('Toxic sentence: ', ' '.join([tokenizer.decode(i, skip_special_tokens = True) for i in input_ids[0]]))\n",
    "  print(f'Detoxified sentence:\\n {tokenizer.decode(outputs[0], skip_special_tokens = True)}')\n",
    "  print('Gold: ', ' '.join([tokenizer.decode(i, skip_special_tokens = True) for i in detoxified_ids[0]]))\n",
    "  print()\n",
    "  if idx == 10:\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
